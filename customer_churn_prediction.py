# -*- coding: utf-8 -*-
"""Customer_Churn_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kLcnXlygFe8ZQ9DyZlxFBNAcD7Qjx_o0

#Understanding the Business Problem

**Defining the problem statement**

Customer churn is a major problem for businesses because losing customers leads to loss of revenue. The company does not clearly know why customers stop using its services. This project aims to analyze customer data to identify the main factors that influence customer churn and help the business understand customer behavior better.

**Need of the study/project**

Help to recover losses early

changes on policies and create some new one so it may attract some new customer

Targeted retention strategies can be created for high-risk customers

**Understanding business**

AlphaCom is facing an increase in customer churn , where many customers are leaving the company. This is a serious issue because losing customers reduces revenue and harms the company position in the market.

customer churn happen because of many reasons such as high price , contract type , not satisfied with company policy and services . So now companie really want to know what is the main reason behind this so they can stop churn.

**Business / Social Opportunity**

By understanding the reasons behind customer churn, the business can take steps to retain existing customers. This creates an opportunity to improve customer satisfaction, reduce revenue loss, and build long-term customer relationships. The insights from this analysis can help the company make better decisions and improve its services for customers.

#Import Required Library
"""

import pandas as pd
import numpy as np

# Library to split data
from sklearn.model_selection import train_test_split
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler

# libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)


# To build model for prediction
import statsmodels.api as SM
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# To tune different models
from sklearn.model_selection import GridSearchCV


# To get diferent metric scores
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    roc_auc_score,
    precision_recall_curve,
    roc_curve,
    make_scorer,
)

from google.colab import drive
drive.mount('/content/drive')

"""**Loading the data**"""

df =pd.read_csv('/content/drive/MyDrive/capston1/customer_churn.csv')

""" Copy Data From Another veriable"""

data = df.copy()

"""#Exploratory Data Analysis

**Data collection and background**
"""

data.head()

data.tail()

"""This dataset contains both numerical and categorical variables.

The target variable in this dataset is Churn, which indicates whether a customer has left the company or not. Since the target variable is already provided, this is a supervised learning problem.

The Churn variable has two categories, Yes and No, which makes it a binary classification problem. Therefore, classification algorithms such as Logistic Regression can be used to predict customer churn.

**Data overview**
"""

data.shape

data.info()

"""In This dataset has 12055 rows and 20 columns.In that 18 is categorical type and 2 is numerical type .There is some missing value in tenure.

Let's check the count of each unique category in each of the categorical variables.
"""

# Making a list of all catrgorical variables
cat_col = list(data.select_dtypes("object").columns)

# Printing number of count of each unique value in each column
for column in cat_col:
    print(data[column].value_counts())
    print("-" * 50)

"""By see each unique category in each of the categorical variables we found 2 issue

TotalCharges show £nan this simbole instent of $
PaymentMethod show diffrent category of most same type due to spelling change so make it correct

Total Charges currency sign fix

**Data Cleaning**
"""

data['TotalCharges'].head(10)

data['TotalCharges'] = data['TotalCharges'].astype(str)

data['TotalCharges'] = data['TotalCharges'].str.replace(r'[$£]', '', regex=True)

data['MonthlyCharges'] = data['MonthlyCharges'].astype(str)

data['MonthlyCharges'] = data['MonthlyCharges'].str.replace(r'[$£]', '', regex=True)

data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')
data['MonthlyCharges'] = pd.to_numeric(data['MonthlyCharges'], errors='coerce')

"""**Univariate Analysis**"""

# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

histogram_boxplot(data, "tenure")

"""The boxplot shows no outliers for Tenure.

Median is low as compair to avg tenure.

It is becouse many of new people leave in mid while loyal customer take tenure high
"""

histogram_boxplot(data, "MonthlyCharges")

histogram_boxplot(data, "TotalCharges")

"""Boxplot shows there is no outlire

Monthly charges show a bimodal distribution, with one cluster around lower charges 10-30 and another around higher charges 100-120.

The median lies slightly above the lower cluster, while the mean is higher due to customers paying premium prices.
TotalCharges is heavily right-skewed, with most customers having low cumulative charges.

A long tail exists for customers with high TotalCharges, representing long tenure and high value customers.

The median is significantly lower than the mean, reflecting that most customers do not stay long enough to accumulate high total spending.
"""

# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        hue=feature,
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

labeled_barplot(data, "gender", perc=True);

labeled_barplot(data, "PaymentMethod", perc=True);

"""**Payment menthod correction**"""

data['PaymentMethod'] = (
    data['PaymentMethod']
    .str.strip()
    .str.lower()
)

labeled_barplot(data, "PaymentMethod", perc=True);

labeled_barplot(data, "PaperlessBilling", perc=True);

labeled_barplot(data, "Contract", perc=True);

labeled_barplot(data, "OnlineBackup", perc=True);

labeled_barplot(data, "Dependents", perc=True);

labeled_barplot(data, "PhoneService", perc=True);

labeled_barplot(data, "InternetService", perc=True);

labeled_barplot(data, "StreamingMovies", perc=True);

labeled_barplot(data, "Churn", perc=True);

"""**fix churn**"""

data['Churn'] = data['Churn'].str.strip().str.capitalize()
data['Churn'].unique()

labeled_barplot(data, "Churn", perc=True);

"""The proportion of male customers is slightly higher compared to female customers.
Around 34% of customers use electronic check as their payment method.
There is no significant difference between customers using paperless billing and those who do not.
Approximately 50.5% of customers do not subscribe to online backup services.
About 23% of customers do not use any internet service.
Nearly 73% of customers do not have dependents.
Around 89% of customers subscribe to phone services, indicating high adoption of basic telecom services.
Among customers who use internet services, fiber optic is the most commonly used internet service type.
The dataset shows a class imbalance, where approximately 71% of customers have not churned, while the remaining customers have churned.

**Observation**

The target variable shows that a larger proportion of customers did not churn, indicating a class imbalance. Customer tenure is right-skewed, meaning many customers have been with the company for a short duration. Monthly charges vary widely, suggesting customers are on different pricing plans.

**Bivariate Analysis**
"""

cols_list = data.select_dtypes(include=np.number).columns.tolist()

plt.figure(figsize=(10, 5))
sns.heatmap(
    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()

"""**Observation**
Customers with month-to-month contracts exhibit significantly higher churn compared to those with long-term contracts.
Higher monthly charges are associated with increased churn, particularly for customers with short tenure.
Customers with short tenure are more likely to churn, indicating that early-stage customers are at higher risk.

**Multivariate analysis**
"""

plt.figure(figsize=(10, 6))
sns.countplot(data=data, x='Contract', hue='Churn', palette='tab10')
plt.title('Churn by Contract Type')
plt.xlabel('Contract Type')
plt.ylabel('Count')
plt.legend(title='Churn')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=data, x='InternetService', hue='Churn', palette='tab10')
plt.title('Churn by Internet Service Type')
plt.xlabel('Internet Service Type')
plt.ylabel('Count')
plt.legend(title='Churn')
plt.show()

plt.figure(figsize=(12, 7))
sns.countplot(data=data, x='PaymentMethod', hue='Churn', palette='tab10')
plt.title('Churn by Payment Method')
plt.xlabel('Payment Method')
plt.ylabel('Count')
plt.legend(title='Churn')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""**Observation**

When combining tenure, contract type, and charges, customers with short tenure, high monthly charges, and flexible contracts show the highest churn rates.
Long-term contracts and longer tenure appear to reduce churn even when monthly charges are relatively high.

**Key Insights from EDA**

Churn is more common among new customers.

Month-to-month contracts are a strong indicator of churn risk.

High monthly charges increase churn risk, especially for customers with short tenure.

Retention strategies should focus on early customer engagement and encouraging long-term contracts.

#Data Preprocessing

**Duplicate value check**
"""

data.duplicated().sum()

"""**Duplicate value treatment**"""

data.drop_duplicates(inplace=True)
print(f" Number of rows after removing duplicates: {data.shape[0]}")

"""**Missing value check**"""

data.isnull().sum()

"""**Missing value treatment**"""

data['tenure'] = data['tenure'].fillna(data['tenure'].median())

data['MonthlyCharges'] = data['MonthlyCharges'].fillna(data['MonthlyCharges'].median())
data['TotalCharges'] = data['TotalCharges'].fillna(data['TotalCharges'].median())

data.isnull().sum()

"""**Data Cleaning**

Remove £ sign from TotalCharges.
Change datatype of TotalCharges and PaymentMethod data type .

**Anomalous value check**
"""

(data['tenure'] < 0).sum()

(data['MonthlyCharges'] < 0).sum()

(data['TotalCharges'] < 0).sum()

"""There are 126 Negtive value of tenure and 147 -ve value in TotalCharges

**Anomalous value treatment**
"""

data.loc[data['tenure'] < 0, 'tenure'] = np.nan
data.loc[data['TotalCharges'] < 0, 'TotalCharges'] = np.nan

data['tenure'] = data['tenure'].fillna(data['tenure'].median())
data['TotalCharges'] = data['TotalCharges'].fillna(data['TotalCharges'].median())

"""**Outlier check**"""

num_cols = data.select_dtypes(include=np.number).columns.tolist()

plt.figure(figsize=(15, 12))

for i, variable in enumerate(num_cols):
    plt.subplot(4, 4, i + 1)
    plt.boxplot(data[variable], whis=1.5)
    plt.tight_layout()
    plt.title(variable)

plt.show()

"""No outliers were found in tenure and MonthlyCharges, as their values are within a reasonable range. The SeniorCitizen variable contains only binary values 0 and 1, so outlier detection is not applicable. Outliers are observed in TotalCharges, but these values are genuine and represent long-term customers with higher service usage. Therefore, these outliers were not treated and were retained for further analysis.

# Task
Engineer two new features in the `data` DataFrame: 'TenureGroup' by bucketing the 'tenure' column into specified time intervals (0-1 year, 1-2 years, 2-4 years, 4-6 years, >6 years) and 'ServiceCount' by summing the number of additional services a customer subscribes to. After creating these features, display the first few rows of the updated `data` DataFrame. Summarize the feature engineering steps and the rationale behind creating these new features.

## Create Tenure Groups

### Subtask:
Engineer a new categorical feature 'TenureGroup' by bucketing the 'tenure' column into meaningful time intervals (e.g., '0-1 year', '1-2 years', '2-4 years', '4-6 years', '>6 years'). This will help capture the impact of customer loyalty over time more effectively.

**Reasoning**:
I will define the bin edges and labels for the 'tenure' column to create the 'TenureGroup' categorical feature. Then, I will use `pd.cut` to apply these bins and labels to the 'tenure' column and store the result in the new 'TenureGroup' column.
"""

bins = [0, 12, 24, 48, 72, 84 , np.inf]
labels = ['0-1 year', '1-2 years', '2-4 years', '4-6 years', '6-8 years' , '> 8years']

data['TenureGroup'] = pd.cut(data['tenure'], bins=bins, labels=labels, right=False)

print(data['TenureGroup'].value_counts())

"""## Create Service Count

### Subtask:
Engineer a new numerical feature 'ServiceCount' by summing the number of additional services a customer subscribes to. This will quantify the level of engagement and potential loyalty based on the number of services used.

#### Instructions:
1. Identify the columns that represent additional services (e.g., 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'MultipleLines', 'PhoneService', 'InternetService').
2. For each identified service column, convert 'Yes' to 1 and 'No' (or 'No internet service', 'No phone service') to 0. You might need to handle 'No internet service' or 'No phone service' carefully as these indicate lack of a service.
3. Sum these converted numerical values to create the 'ServiceCount' for each customer.

**Reasoning**:
Following the previous step, I will now engineer the 'ServiceCount' feature by converting the identified service-related categorical columns into numerical (binary) representations and then summing them up to quantify the number of services each customer subscribes to.
"""

service_cols = [
    'PhoneService',
    'MultipleLines',
    'InternetService',
    'OnlineSecurity',
    'OnlineBackup',
    'DeviceProtection',
    'TechSupport',
    'StreamingTV',
    'StreamingMovies'
]

# Create a temporary DataFrame with only the service columns to perform the mapping
service_mapping_df = data[service_cols].copy()

# Replace 'Yes' with 1, and all 'No' variants and service types with 0 or 1 appropriately
service_mapping_df = service_mapping_df.replace({
    'Yes': 1,
    'No': 0,
    'No phone service': 0,
    'No internet service': 0,
    'Fiber optic': 1, # InternetService type
    'DSL': 1          # InternetService type
})

# Convert all columns to numeric, coercing any non-convertible values to NaN (though none should exist after the above replacements)
for col in service_cols:
    service_mapping_df[col] = pd.to_numeric(service_mapping_df[col], errors='coerce')

# Sum the binary service columns to create 'ServiceCount'
data['ServiceCount'] = service_mapping_df[service_cols].sum(axis=1)

print("First 5 rows of data with new 'ServiceCount' feature:")
print(data[['gender', 'tenure', 'TenureGroup'] + service_cols + ['ServiceCount']].head())
print("\nValue counts for 'ServiceCount':")
print(data['ServiceCount'].value_counts().sort_index())

print(data[['tenure', 'TenureGroup', 'MonthlyCharges', 'TotalCharges', 'ServiceCount', 'Churn']].head())

"""**Class Imbalance Handling**"""

data['Churn'].value_counts(normalize=True)

"""**Feature engineering**"""

data['Churn'] = data['Churn'].map({'No': 0, 'Yes': 1})

X = data.drop('Churn', axis=1)
y = data['Churn']

X = pd.get_dummies(X, dtype=int, columns=X.select_dtypes(include=['object', 'category']).columns.tolist(), drop_first=True)
X.head()

"""**Data Preparation for Modeling**

Train test and validate split to prevent data lekage
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)

print(X_train.shape, X_val.shape, X_test.shape)
print(y_train.shape, y_val.shape, y_test.shape)

"""**Feature scaling**"""

scaler = StandardScaler()

# Fit only on training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform validation and test data
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

"""#Model Building - Baseline Model

Recall was selected as the primary evaluation metric because the objective is to correctly identify customers who are likely to churn. Due to class imbalance, accuracy alone may be misleading.
"""

def model_performance_classification(model, predictors, target, threshold=0.5):
    """
    Function to compute classification model performance metrics
    """

    # Get predicted probabilities for class 1
    prob_pred = model.predict_proba(predictors)[:, 1]

    # Apply threshold
    class_pred = (prob_pred >= threshold).astype(int)

    acc = accuracy_score(target, class_pred)
    recall = recall_score(target, class_pred)
    precision = precision_score(target, class_pred)
    f1 = f1_score(target, class_pred)

    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf

def plot_confusion_matrix(model, predictors, target, threshold=0.5):
    """
    Plot confusion matrix with percentages
    """

    prob_pred = model.predict_proba(predictors)[:, 1]
    class_pred = (prob_pred >= threshold).astype(int)

    cm = confusion_matrix(target, class_pred)

    labels = np.asarray(
        [
            ["{0:d}\n{1:.2%}".format(item, item / cm.sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="", cmap="Blues")
    plt.ylabel("Actual")
    plt.xlabel("Predicted")
    plt.show()

"""**Build a Baseline Model (Logistic Regression)**"""

baseline_model = LogisticRegression(
    class_weight='balanced',
    max_iter=1000,
    random_state=42
)

baseline_model.fit(X_train_scaled, y_train)

model_performance_classification(
    model=baseline_model,
    predictors=X_val_scaled,
    target=y_val,
    threshold=0.5
)

"""threshold=0.35"""

model_performance_classification(
    model=baseline_model,
    predictors=X_val_scaled,
    target=y_val,
    threshold=0.35
)

"""Confusion Matrix"""

plot_confusion_matrix(
    model=baseline_model,
    predictors=X_val_scaled,
    target=y_val,
    threshold=0.5
)

"""**threshold=0.5**"""

model_performance_classification(
    model=baseline_model,
    predictors=X_test_scaled,
    target=y_test,
    threshold=0.5
)

"""**Insights**

Customers who are new to the service are more likely to leave.

Customers on month-to-month contracts have higher churn than those on long-term contracts.

Higher monthly charges increase the chance of churn, especially for new customers.

The model shows that tenure, contract type, and monthly charges are the main factors affecting churn.

Using class-balanced logistic regression helps the model identify more churned customers.

**Recommendations**

Focus on retaining customers during the first few months of service.

Encourage customers to switch from month-to-month plans to longer contracts by offering discounts.

Review pricing plans for customers with high monthly charges.

Use the model to identify customers at risk of churn and reach out to them early.

In future work, try more advanced models to improve churn prediction.
"""